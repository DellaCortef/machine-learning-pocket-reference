{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecf1ddcd",
   "metadata": {},
   "source": [
    "### Instaling libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd9b837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade rfpimp\n",
    "# ! pip install --upgrade scikit-learn\n",
    "# ! pip install --upgrade pyarrow\n",
    "# ! pip install --upgrade yellowbrick\n",
    "# ! pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1481cac8",
   "metadata": {},
   "source": [
    "### Column description\n",
    "\n",
    "| Variável \t| Descrição \t|\n",
    "|:-:\t|:-\t|\n",
    "| PassangerID \t| ID de identificação do passageiro(a) \t|\n",
    "| Survived \t| se o passageiro(a) sobreviveu (0 = não, 1 = sim) \t|\n",
    "| Pclass \t| classe do passageiro:<br>     * **1 = primeira**,<br>     * **2 = segunda**,<br>     * **3 = terceira** \t|\n",
    "| name \t| nome do passageiro(a) \t|\n",
    "| sex \t| sexo do passageiro(a) \t|\n",
    "| age \t| idade do passageiro(a) \t|\n",
    "| Sibsp \t| número de irmão(ãs)/esposo(a) à bordo \t|\n",
    "| Parch \t| número de pais/filhos(as) à bordo \t|\n",
    "| Ticket \t| número da passagem \t|\n",
    "| Fare \t| preço da passagem \t|\n",
    "| Cabin \t| cabine \t|\n",
    "| Embarked \t| local que o passageiro(a) embarcou:<br>     * **C = Cherboug**,<br>     * **Q = Queenstown**,<br>     * **S = Southamption** \t|\n",
    "| WikiId \t| ID de identificação do passageiro(a) segundo Wikipedia \t|\n",
    "| Name_wiki \t| nome do passageiro(a) \t|\n",
    "| Age_wiki \t| idade do passageiro(a) \t|\n",
    "| Hometown \t| cidade de nascimento do passageiro(a) \t|\n",
    "| Boarded \t| cidade de embarque \t|\n",
    "| Destination \t| destino da viagem \t|\n",
    "| Lifeboat \t| identificação do bote salva-vidas \t|\n",
    "| Body \t| número de identificação do corpo \t|\n",
    "\n",
    "\n",
    "<font color='red'>**IMPORTANT**</font>\n",
    "\n",
    "The new features (the ones after 'Embarked') are very similar to the original ones but they are more up-to-date and have much fewer missing values. Therefore, users can decide on the preferred features themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8ea5be",
   "metadata": {},
   "source": [
    "### Importing Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e99101d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# data visualization\n",
    "import rfpimp\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from seaborn import (\n",
    "    jointplot,\n",
    "    pairplot,\n",
    "    boxplot,\n",
    "    heatmap\n",
    ")\n",
    "from yellowbrick.features import (\n",
    "    RFECV,\n",
    "    Rank2D, \n",
    "    RadViz,\n",
    "    ParallelCoordinates,\n",
    "    JointPlotVisualizer\n",
    ")\n",
    "\n",
    "# data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import(\n",
    "    radviz\n",
    ")\n",
    "import janitor as jn\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# missing values\n",
    "import missingno as msno\n",
    "\n",
    "from sklearn.impute import (\n",
    "    SimpleImputer\n",
    ")\n",
    "\n",
    "# machine learning models\n",
    "from sklearn import (\n",
    "    tree,\n",
    "    impute,\n",
    "    ensemble,\n",
    "    linear_model,\n",
    "    preprocessing,\n",
    "    model_selection,\n",
    "    feature_selection,\n",
    ")\n",
    "\n",
    "from sklearn.feature_selection import (\n",
    "    RFE\n",
    ")\n",
    "\n",
    "from sklearn.dummy import (\n",
    "    DummyClassifier\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split\n",
    ")\n",
    "\n",
    "from sklearn.experimental import (\n",
    "    enable_iterative_imputer\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression\n",
    ")\n",
    "\n",
    "from sklearn.tree import (\n",
    "    DecisionTreeClassifier\n",
    ")\n",
    "\n",
    "from sklearn.neighbors import (\n",
    "    KNeighborsClassifier\n",
    ")\n",
    "\n",
    "from sklearn.naive_bayes import (\n",
    "    GaussianNB\n",
    ")\n",
    "\n",
    "from sklearn.svm import (\n",
    "    SVC\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier\n",
    ")\n",
    "\n",
    "import xgboost\n",
    "\n",
    "# data model metrics\n",
    "from sklearn.metrics import (\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "\n",
    "from yellowbrick.classifier import (\n",
    "    ConfusionMatrix\n",
    ")\n",
    "\n",
    "from yellowbrick.model_selection import (\n",
    "    LearningCurve\n",
    ")\n",
    "\n",
    "# data prep-model\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    "    learning_curve\n",
    ")\n",
    "\n",
    "# model deploy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7386fc97",
   "metadata": {},
   "source": [
    "### Reading the Titanin Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8b23eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"titanic_dataset.csv\", index_col=0)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b940cbc",
   "metadata": {},
   "source": [
    "### Deleting _Class_ feature at the end\n",
    "We are deleting because is the same as _pclass_ (same result, same data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e77716",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Class', axis = 'columns')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccd2c7b",
   "metadata": {},
   "source": [
    "### Converting DataFrame Column Names to Lowercase snakecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808f2ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = (df.columns\n",
    "                .str.replace('(?<=[a-z])(?=[A-Z])', '_', regex=True)\n",
    "                .str.lower()\n",
    "             )\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21fff0c",
   "metadata": {},
   "source": [
    "### Data Engineering\n",
    "We can use pandas to generate new attributes. Sometimes, we can receive these transformations from the Engineering team, but other times, we need to create these visions ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50893a6e",
   "metadata": {},
   "source": [
    "#### Aggregate\n",
    "We can aggregate cabin data, class, ticket price, among others, with age (average, mode, maximum, minimum, ...). For this, we will use .groupby() method from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49746ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_cabin = df.drop(columns = ['name',\n",
    "'ticket',\n",
    "'sex',\n",
    "#'cabin',\n",
    "'embarked',\n",
    "'name_wiki',\n",
    "'hometown',\n",
    "'boarded',\n",
    "'destination',\n",
    "'lifeboat',\n",
    "'body'])\n",
    "\n",
    "orig_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f558774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cabin feature\n",
    "agg = (\n",
    "    df_agg_cabin.groupby(\"cabin\")\n",
    "    .agg(\"min,max,mean,sum\".split(\",\"))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "agg.columns = [\n",
    "    \"_\".join(c).strip(\"_\")\n",
    "    for c in agg.columns.values\n",
    "]\n",
    "\n",
    "agg_df = df_agg_cabin.merge(agg, on = \"cabin\")\n",
    "\n",
    "agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5b891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the feature types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a5ac8",
   "metadata": {},
   "source": [
    "### Attribute selection\n",
    "We use feature selection (which is part of feature engineering) to, after some analysis, choose which attributes should be part of our models.<br>\n",
    "It is important to keep in mind that some attributes can have a negative effect on prediction models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a11ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweak_titanic(df):\n",
    "    df = df.drop(\n",
    "        columns=[\n",
    "            \"name\",\n",
    "            \"name_wiki\",\n",
    "            \"wiki_id\",\n",
    "            \"age_wiki\",\n",
    "            \"ticket\",\n",
    "            \"destination\",\n",
    "            \"body\",\n",
    "            \"cabin\",\n",
    "            \"hometown\",\n",
    "            \"lifeboat\",\n",
    "            \"boarded\"\n",
    "        ]\n",
    "    ).pipe(pd.get_dummies, drop_first=True)\n",
    "    return df\n",
    "\n",
    "def get_train_test_X_y(\n",
    "    df, y_col, size=0.3, std_cols=None\n",
    "):\n",
    "    y = df[y_col]\n",
    "    X = df.drop(columns=y_col)\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "        X, y, test_size=size, random_state=42\n",
    "    )\n",
    "    cols = X.columns\n",
    "    num_cols = [\n",
    "        \"pclass\",\n",
    "        \"age\",\n",
    "        \"sib_sp\",\n",
    "        \"parch\",\n",
    "        \"fare\",\n",
    "    ]\n",
    "    fi = impute.IterativeImputer()\n",
    "    fitted = fi.fit_transform(X_train[num_cols])\n",
    "    X_train = X_train.assign(**{c:fitted[:,i] for i, c in enumerate(num_cols)})\n",
    "    test_fit = fi.transform(X_test[num_cols])\n",
    "    X_test = X_test.assign(**{c:test_fit[:,i] for i, c in enumerate(num_cols)})\n",
    "    if std_cols:\n",
    "        std = preprocessing.StandardScaler()\n",
    "        fitted = std.fit_transform(X_train[std_cols])\n",
    "        X_train = X_train.assign(**{c:fitted[:,i] for i, c in enumerate(std_cols)})\n",
    "        test_fit = std.transform(X_test[std_cols])\n",
    "        X_test = X_test.assign(**{c:test_fit[:,i] for i, c in enumerate(std_cols)})\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "ti_df = tweak_titanic(orig_df)\n",
    "std_cols = \"pclass,age,sib_sp,fare\".split(\",\")\n",
    "X_train, X_test, y_train, y_test = get_train_test_X_y(\n",
    "    ti_df, \"survived\", std_cols=std_cols\n",
    ")\n",
    "\n",
    "X = pd.concat([X_train, X_test])\n",
    "y = pd.concat([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a5b8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = agg_df.drop(columns = ['cabin'])\n",
    "\n",
    "agg_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcff264",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 0.95\n",
    "corr = agg_df.corr()\n",
    "mask = np.triu(\n",
    "    np.ones(corr.shape), k=1\n",
    ").astype(bool)\n",
    "corr_no_diag = corr.where(mask)\n",
    "coll = [\n",
    "    c\n",
    "    for c in corr_no_diag.columns\n",
    "    if any(abs(corr_no_diag[c]) > limit)\n",
    "]\n",
    "coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ed30ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b4bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2acbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfpimp.plot_dependence_heatmap(\n",
    "    rfpimp.feature_dependence_matrix(X_train),\n",
    "    value_fontsize=12,\n",
    "    label_fontsize=14,\n",
    "    figsize=(8, 8)\n",
    ")\n",
    "fig = plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258cb2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns taht do not add value\n",
    "df_prep = df.drop(columns = ['name',\n",
    "                             'name_wiki',\n",
    "                             'wiki_id',\n",
    "                             'hometown',\n",
    "                             'destination',\n",
    "                             'ticket',\n",
    "                             'lifeboat',\n",
    "                             'body',\n",
    "                             'cabin',\n",
    "                             'boarded',\n",
    "                             'age'])\n",
    "\n",
    "# using get_dummies function to convert object to int\n",
    "df_prep = pd.get_dummies(df_prep)\n",
    "\n",
    "# dropping redundant features\n",
    "df_prep = df_prep.drop(columns = ['sex_male'])\n",
    "\n",
    "#remove rows with any values that are not finite (NaN or infite)\n",
    "df_prep = df_prep[np.isfinite(df_prep).all(1)]\n",
    "\n",
    "# first, we need to create a series of the target feature\n",
    "y = df_prep.survived\n",
    "\n",
    "# then, we create a DataFrame with the attributes\n",
    "X = df_prep.drop(columns = ['survived'])\n",
    "\n",
    "# using the scikit-learn to split 30% to test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e6449",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LassoLarsCV(\n",
    "    cv = 10, max_n_alphas = 10\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "cm = iter(\n",
    "    plt.get_cmap(\"tab20\")(\n",
    "        np.linspace(0, 1, X.shape[1])\n",
    "    )\n",
    ")\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    c = next(cm)\n",
    "    ax.plot(\n",
    "        model.alphas_,\n",
    "        model.coef_path_.T[:, i],\n",
    "        c = c,\n",
    "        alpha = 0.8,\n",
    "        label = X.columns[i],\n",
    "    )\n",
    "ax.axvline(\n",
    "    model.alpha_,\n",
    "    linestyle = \"-\",\n",
    "    c = \"k\",\n",
    "    label = \"alphaCV\",\n",
    ")\n",
    "\n",
    "plt.ylabel(\"Regression Coefficients\")\n",
    "ax.legend(X.columns, bbox_to_anchor=(1, 1))\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.title(\n",
    "    \"Regression Coefficients Progression for Lasso Paths\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd42e75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "rfe = RFECV(\n",
    "    ensemble.RandomForestClassifier(\n",
    "        n_estimators = 100\n",
    "    ),\n",
    "    cv = 5,\n",
    ")\n",
    "\n",
    "rfe.fit(X, y)\n",
    "\n",
    "rfe.rfe_estimator_.ranking_\n",
    "\n",
    "rfe.rfe_estimator_.n_features_\n",
    "\n",
    "rfe.rfe_estimator_.support_\n",
    "\n",
    "rfe.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5cc775",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ensemble.RandomForestClassifier(\n",
    "    n_estimators=100\n",
    ")\n",
    "rfe = RFE(model)\n",
    "rfe.fit(X, y)\n",
    "X.columns[rfe.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f057e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mic = feature_selection.mutual_info_classif(\n",
    "    X, y\n",
    ")\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "(\n",
    "    pd.DataFrame(\n",
    "        {\"feature\": X.columns, \"vimp\": mic}\n",
    "    )\n",
    "    .set_index(\"feature\")\n",
    "    .plot.barh(ax=ax)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb8ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
