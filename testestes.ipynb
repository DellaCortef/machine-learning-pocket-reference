{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0caa1a0f",
   "metadata": {},
   "source": [
    "### Intalling libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74e34be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade rfpimp\n",
    "# ! pip install --upgrade scikit-learn\n",
    "# ! pip install --upgrade pyarrow\n",
    "# ! pip install --upgrade yellowbrick\n",
    "# ! pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d239cc03",
   "metadata": {},
   "source": [
    "### Column description\n",
    "\n",
    "| Variável \t| Descrição \t|\n",
    "|:-:\t|:-\t|\n",
    "| PassangerID \t| ID de identificação do passageiro(a) \t|\n",
    "| Survived \t| se o passageiro(a) sobreviveu (0 = não, 1 = sim) \t|\n",
    "| Pclass \t| classe do passageiro:<br>     * **1 = primeira**,<br>     * **2 = segunda**,<br>     * **3 = terceira** \t|\n",
    "| name \t| nome do passageiro(a) \t|\n",
    "| sex \t| sexo do passageiro(a) \t|\n",
    "| age \t| idade do passageiro(a) \t|\n",
    "| Sibsp \t| número de irmão(ãs)/esposo(a) à bordo \t|\n",
    "| Parch \t| número de pais/filhos(as) à bordo \t|\n",
    "| Ticket \t| número da passagem \t|\n",
    "| Fare \t| preço da passagem \t|\n",
    "| Cabin \t| cabine \t|\n",
    "| Embarked \t| local que o passageiro(a) embarcou:<br>     * **C = Cherboug**,<br>     * **Q = Queenstown**,<br>     * **S = Southamption** \t|\n",
    "| WikiId \t| ID de identificação do passageiro(a) segundo Wikipedia \t|\n",
    "| Name_wiki \t| nome do passageiro(a) \t|\n",
    "| Age_wiki \t| idade do passageiro(a) \t|\n",
    "| Hometown \t| cidade de nascimento do passageiro(a) \t|\n",
    "| Boarded \t| cidade de embarque \t|\n",
    "| Destination \t| destino da viagem \t|\n",
    "| Lifeboat \t| identificação do bote salva-vidas \t|\n",
    "| Body \t| número de identificação do corpo \t|\n",
    "\n",
    "\n",
    "<font color='red'>**IMPORTANT**</font>\n",
    "\n",
    "The new features (the ones after 'Embarked') are very similar to the original ones but they are more up-to-date and have much fewer missing values. Therefore, users can decide on the preferred features themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696865d3",
   "metadata": {},
   "source": [
    "### Importing Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c7b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data visualization\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import (\n",
    "    jointplot,\n",
    "    pairplot,\n",
    "    boxplot,\n",
    "    heatmap\n",
    ")\n",
    "from yellowbrick.features import (\n",
    "    JointPlotVisualizer,\n",
    "    Rank2D, \n",
    "    RadViz,\n",
    "    ParallelCoordinates\n",
    ")\n",
    "\n",
    "# data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import(\n",
    "    radviz\n",
    ")\n",
    "import janitor as jn\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# missing values\n",
    "import missingno as msno\n",
    "\n",
    "from sklearn.impute import (\n",
    "    SimpleImputer\n",
    ")\n",
    "\n",
    "# machine learning models\n",
    "from sklearn import (\n",
    "    ensemble,\n",
    "    preprocessing,\n",
    "    tree,\n",
    "    impute,\n",
    "    model_selection\n",
    ")\n",
    "\n",
    "from sklearn.dummy import (\n",
    "    DummyClassifier\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split\n",
    ")\n",
    "\n",
    "from sklearn.experimental import (\n",
    "    enable_iterative_imputer\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression\n",
    ")\n",
    "\n",
    "from sklearn.tree import (\n",
    "    DecisionTreeClassifier\n",
    ")\n",
    "\n",
    "from sklearn.neighbors import (\n",
    "    KNeighborsClassifier\n",
    ")\n",
    "\n",
    "from sklearn.naive_bayes import (\n",
    "    GaussianNB\n",
    ")\n",
    "\n",
    "from sklearn.svm import (\n",
    "    SVC\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier\n",
    ")\n",
    "\n",
    "import xgboost\n",
    "\n",
    "# data model metrics\n",
    "from sklearn.metrics import (\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "\n",
    "from yellowbrick.classifier import (\n",
    "    ConfusionMatrix\n",
    ")\n",
    "\n",
    "from yellowbrick.model_selection import (\n",
    "    LearningCurve\n",
    ")\n",
    "\n",
    "# data prep-model\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    "    learning_curve\n",
    ")\n",
    "\n",
    "# model deploy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e460a7d",
   "metadata": {},
   "source": [
    "### Reading the Titanic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f47c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"titanic_dataset.csv\", index_col=0)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08aa73e",
   "metadata": {},
   "source": [
    "### Deleting _Class_ feature at the end\n",
    "We are deleting because is the same as _pclass_ (same result, same data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a06118",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Class', axis = 'columns')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9a0c5f",
   "metadata": {},
   "source": [
    "### Converting DataFrame Column Names to Lowercase snakecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c34f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = (df.columns\n",
    "                .str.replace('(?<=[a-z])(?=[A-Z])', '_', regex=True)\n",
    "                .str.lower()\n",
    "             )\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a274ef",
   "metadata": {},
   "source": [
    "### Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c575a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# dropping columns taht do not add value\n",
    "df_heatmap_collinearity = df.drop(columns = ['name',\n",
    "                        'name_wiki',\n",
    "                        'wiki_id',\n",
    "                        'hometown',\n",
    "                        'destination',\n",
    "                        'ticket',\n",
    "                        'lifeboat',\n",
    "                        'body',\n",
    "                        'age'])\n",
    "\n",
    "# using get_dummies function to convert object to int\n",
    "df_heatmap_collinearity = pd.get_dummies(df)\n",
    "\n",
    "# dropping redundant features\n",
    "df_heatmap_collinearity = df_heatmap_collinearity.drop(columns = ['sex_male'])\n",
    "\n",
    "#remove rows with any values that are not finite (NaN or infite)\n",
    "df_heatmap_collinearity = df_heatmap_collinearity[np.isfinite(df_heatmap_collinearity).all(1)]\n",
    "\n",
    "# first, we need to create a series of the target feature\n",
    "y = df_heatmap_collinearity.survived\n",
    "\n",
    "# then, we create a DataFrame with the attributes\n",
    "X = df_heatmap_collinearity.drop(columns = ['survived'])\n",
    "\n",
    "# using the scikit-learn to split 30% to test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cdd20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_fea_eng = df.drop(columns = ['name',\n",
    "'ticket',\n",
    "'sex',\n",
    "#'cabin',\n",
    "'embarked',\n",
    "'name_wiki',\n",
    "'hometown',\n",
    "'boarded',\n",
    "'destination',\n",
    "'lifeboat',\n",
    "'body']'\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d92374",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "We use feature selection to select variables that are useful for the model. Irrelevant attributes can have a negative effect on our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe831c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweak_titanic(df):\n",
    "    df = df.drop(\n",
    "        columns=[\n",
    "            \"name\",\n",
    "            \"ticket\",\n",
    "            \"body\",\n",
    "            \"cabin\",\n",
    "        ]\n",
    "    ).pipe(pd.get_dummies, drop_first=True)\n",
    "    return df\n",
    "\n",
    "def get_train_test_X_y(\n",
    "    df, y_col, size=0.3, std_cols=None\n",
    "):\n",
    "    y = df[y_col]\n",
    "    X = df.drop(columns=y_col)\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "        X, y, test_size=size, random_state=42\n",
    "    )\n",
    "    cols = X.columns\n",
    "    num_cols = [\n",
    "        \"pclass\",\n",
    "        \"age_wiki\",\n",
    "        \"sib_sp\",\n",
    "        \"parch\",\n",
    "        \"fare\",\n",
    "    ]\n",
    "    fi = impute.IterativeImputer()\n",
    "    fitted = fi.fit_transform(X_train[num_cols])\n",
    "    X_train = X_train.assign(**{c:fitted[:,i] for i, c in enumerate(num_cols)})\n",
    "    test_fit = fi.transform(X_test[num_cols])\n",
    "    X_test = X_test.assign(**{c:test_fit[:,i] for i, c in enumerate(num_cols)})\n",
    "    if std_cols:\n",
    "        std = preprocessing.StandardScaler()\n",
    "        fitted = std.fit_transform(X_train[std_cols])\n",
    "        X_train = X_train.assign(**{c:fitted[:,i] for i, c in enumerate(std_cols)})\n",
    "        test_fit = std.transform(X_test[std_cols])\n",
    "        X_test = X_test.assign(**{c:test_fit[:,i] for i, c in enumerate(std_cols)})\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "orig_df = df\n",
    "\n",
    "ti_df = tweak_titanic(orig_df)\n",
    "std_cols = \"pclass,age_wiki,sib_sp,fare\".split(\",\")\n",
    "X_train, X_test, y_train, y_test = get_train_test_X_y(\n",
    "    ti_df, \"survived\", std_cols=std_cols\n",
    ")\n",
    "\n",
    "X = pd.concat([X_train, X_test])\n",
    "y = pd.concat([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903fc614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rfpimp\n",
    "rfpimp.plot_dependence_heatmap(\n",
    "    rfpimp.feature_dependence_matrix(X_train),\n",
    "    value_fontsize=12,\n",
    "    label_fontsize=14,\n",
    "    figsize=(8, 8)\n",
    ")\n",
    "fig = plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba36e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cabin feature\n",
    "agg = (\n",
    "    df_fea_eng.groupby(\"cabin\")\n",
    "    .agg(\"min,max,mean,sum\".split(\",\"))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "agg.columns = [\n",
    "    \"_\".join(c).strip(\"_\")\n",
    "    for c in agg.columns.values\n",
    "]\n",
    "\n",
    "agg_df = df_fea_eng.merge(agg, on = \"cabin\")\n",
    "\n",
    "agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cdcbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98146491",
   "metadata": {},
   "source": [
    "#### Collinear columns\n",
    "We can define a collinearity limit and run code that will return features above that limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d24dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = agg_df.drop(columns = ['cabin'])\n",
    "\n",
    "limit = 0.95\n",
    "corr = agg_df.corr()\n",
    "\n",
    "mask = np.triu(\n",
    "    np.ones(corr.shape), k = 1\n",
    ").astype(bool)\n",
    "\n",
    "corr_no_diag = corr.where(mask)\n",
    "\n",
    "coll = [\n",
    "    c\n",
    "    for c in corr_no_diag.columns\n",
    "    if any(abs(corr_no_diag[c]) > limit)\n",
    "]\n",
    "\n",
    "coll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed1fe6",
   "metadata": {},
   "source": [
    "### Recursive attribute elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f798b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cacec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
