{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0caa1a0f",
   "metadata": {},
   "source": [
    "### Intalling libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74e34be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d239cc03",
   "metadata": {},
   "source": [
    "### Column description\n",
    "\n",
    "| Variável \t| Descrição \t|\n",
    "|:-:\t|:-\t|\n",
    "| PassangerID \t| ID de identificação do passageiro(a) \t|\n",
    "| Survived \t| se o passageiro(a) sobreviveu (0 = não, 1 = sim) \t|\n",
    "| Pclass \t| classe do passageiro:<br>     * **1 = primeira**,<br>     * **2 = segunda**,<br>     * **3 = terceira** \t|\n",
    "| name \t| nome do passageiro(a) \t|\n",
    "| sex \t| sexo do passageiro(a) \t|\n",
    "| age \t| idade do passageiro(a) \t|\n",
    "| Sibsp \t| número de irmão(ãs)/esposo(a) à bordo \t|\n",
    "| Parch \t| número de pais/filhos(as) à bordo \t|\n",
    "| Ticket \t| número da passagem \t|\n",
    "| Fare \t| preço da passagem \t|\n",
    "| Cabin \t| cabine \t|\n",
    "| Embarked \t| local que o passageiro(a) embarcou:<br>     * **C = Cherboug**,<br>     * **Q = Queenstown**,<br>     * **S = Southamption** \t|\n",
    "| WikiId \t| ID de identificação do passageiro(a) segundo Wikipedia \t|\n",
    "| Name_wiki \t| nome do passageiro(a) \t|\n",
    "| Age_wiki \t| idade do passageiro(a) \t|\n",
    "| Hometown \t| cidade de nascimento do passageiro(a) \t|\n",
    "| Boarded \t| cidade de embarque \t|\n",
    "| Destination \t| destino da viagem \t|\n",
    "| Lifeboat \t| identificação do bote salva-vidas \t|\n",
    "| Body \t| número de identificação do corpo \t|\n",
    "\n",
    "\n",
    "<font color='red'>**IMPORTANT**</font>\n",
    "\n",
    "The new features (the ones after 'Embarked') are very similar to the original ones but they are more up-to-date and have much fewer missing values. Therefore, users can decide on the preferred features themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696865d3",
   "metadata": {},
   "source": [
    "### Importing Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71c7b8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# data visualization\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import (\n",
    "    jointplot,\n",
    "    pairplot,\n",
    "    boxplot,\n",
    "    heatmap\n",
    ")\n",
    "from yellowbrick.features import (\n",
    "    JointPlotVisualizer,\n",
    "    Rank2D, \n",
    "    RadViz,\n",
    "    ParallelCoordinates\n",
    ")\n",
    "\n",
    "# data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import(\n",
    "    radviz\n",
    ")\n",
    "\n",
    "from collections import (\n",
    "    Counter,\n",
    ")\n",
    "\n",
    "import janitor as jn\n",
    "\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# missing values\n",
    "import missingno as msno\n",
    "\n",
    "from sklearn.impute import (\n",
    "    SimpleImputer\n",
    ")\n",
    "\n",
    "# machine learning models\n",
    "from sklearn import (\n",
    "    ensemble,\n",
    "    preprocessing,\n",
    "    tree,\n",
    "    impute,\n",
    "    model_selection,\n",
    "    preprocessing\n",
    ")\n",
    "\n",
    "from sklearn.dummy import (\n",
    "    DummyClassifier\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split\n",
    ")\n",
    "\n",
    "from sklearn.experimental import (\n",
    "    enable_iterative_imputer\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression\n",
    ")\n",
    "\n",
    "from sklearn.tree import (\n",
    "    DecisionTreeClassifier\n",
    ")\n",
    "\n",
    "from sklearn.neighbors import (\n",
    "    KNeighborsClassifier\n",
    ")\n",
    "\n",
    "from sklearn.naive_bayes import (\n",
    "    GaussianNB\n",
    ")\n",
    "\n",
    "from sklearn.svm import (\n",
    "    SVC\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier\n",
    ")\n",
    "\n",
    "import xgboost\n",
    "\n",
    "# data model metrics\n",
    "from sklearn.metrics import (\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "\n",
    "from yellowbrick.classifier import (\n",
    "    ConfusionMatrix\n",
    ")\n",
    "\n",
    "from yellowbrick.model_selection import (\n",
    "    LearningCurve\n",
    ")\n",
    "\n",
    "# data prep-model\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    "    learning_curve\n",
    ")\n",
    "\n",
    "# model deploy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e460a7d",
   "metadata": {},
   "source": [
    "### Reading the Titanic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f47c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"titanic_dataset.csv\", index_col=0)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08aa73e",
   "metadata": {},
   "source": [
    "### Deleting _Class_ feature at the end\n",
    "We are deleting because is the same as _pclass_ (same result, same data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a06118",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Class', axis = 'columns')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9a0c5f",
   "metadata": {},
   "source": [
    "### Converting DataFrame Column Names to Lowercase snakecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c34f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = (df.columns\n",
    "                .str.replace('(?<=[a-z])(?=[A-Z])', '_', regex=True)\n",
    "                .str.lower()\n",
    "             )\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a274ef",
   "metadata": {},
   "source": [
    "### Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c575a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns taht do not add value\n",
    "df = df.drop(columns = ['name',\n",
    "                        'name_wiki',\n",
    "                        'wiki_id',\n",
    "                        'hometown',\n",
    "                        'destination',\n",
    "                        'ticket',\n",
    "                        'lifeboat',\n",
    "                        'body',\n",
    "                        'cabin',\n",
    "                        'age'])\n",
    "\n",
    "# using get_dummies function to convert object to float\n",
    "df = pd.get_dummies(df, dtype = float)\n",
    "\n",
    "# dropping redundant features\n",
    "df = df.drop(columns = ['sex_male'])\n",
    "\n",
    "#remove rows with any values that are not finite (NaN or infite)\n",
    "df = df[np.isfinite(df).all(1)]\n",
    "\n",
    "# first, we need to create a series of the target feature\n",
    "y = df.survived\n",
    "\n",
    "# then, we create a DataFrame with the attributes\n",
    "X = df.drop(columns = ['survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af977d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9507c5a",
   "metadata": {},
   "source": [
    "### Standardizing the Data\n",
    "Some Machine Learning algorithms perform better when the data is standardized, that is, each feature must have mean = 0 and standard deviation = 1<br>\n",
    "In this case, we will use the StandardScaler from sklearn lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06313f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying the dataset\n",
    "df_std = df.copy()\n",
    "\n",
    "# assigning StandardScaler to a variable\n",
    "std = preprocessing.StandardScaler()\n",
    "\n",
    "# applying the standard scaler\n",
    "std.fit_transform(df_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff8f546",
   "metadata": {},
   "source": [
    "#### Some attributes of StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd7d057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean\n",
    "std.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6352546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance\n",
    "std.var_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b394b6",
   "metadata": {},
   "source": [
    "### Min-Max-Scaler\n",
    "Min-Max Scaling, also known as scaling normalization, is another widely used Feature Scaling technique. In this approach, feature values are set to a specific range, usually between 0 and 1.<br>\n",
    "We are not going to use MinMaxScaler on the dataset, but we will leave the code for future possibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d473ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying the dataset\n",
    "df_mms = df.copy()\n",
    "\n",
    "# assigning MinMaxScaler to a variable\n",
    "mms = preprocessing.MinMaxScaler()\n",
    "\n",
    "# applying the minimum and maximum scaler\n",
    "mms.fit_transform(df_mms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e0f19",
   "metadata": {},
   "source": [
    "### Dummies features\n",
    "Dummy variables are binary variables (0 or 1) created to represent a variable with two or more categories.<br>\n",
    "Dummy variables must be used whenever we wish to include categorical variables in models that only accept numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389d27fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using get_dummies function to convert object to boolean\n",
    "df = pd.get_dummies(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05229d8",
   "metadata": {},
   "source": [
    "Note that variables that were previously _object_ are now of type _boolean_.\n",
    "We can convert the features to float type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f2501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using get_dummies function to convert object to float\n",
    "df = pd.get_dummies(df, dtype=float)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662cc778",
   "metadata": {},
   "source": [
    "### Label Encoder\n",
    "An alternative to dummy variable coding is label coding. In this case, each category data will be assigned a number. It is a convenient method for data with high cardinality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1639b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying the dataset\n",
    "df_lbe = df['pclass'].copy()\n",
    "\n",
    "# assigning LabelEncoder to a variable\n",
    "labenc = preprocessing.LabelEncoder()\n",
    "\n",
    "# applying the label encoder\n",
    "labenc.fit_transform(df_lbe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6129ecdd",
   "metadata": {},
   "source": [
    "We can decode the LabelEncoder from the encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a23e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the inverse codification\n",
    "labenc.inverse_transform([2, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9eee21",
   "metadata": {},
   "source": [
    "### Extracting Categories from Strings\n",
    "One of the ways to increase the accuracy of models is to extract the titles of names.<br>\n",
    "We can use the Counter class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf244d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter()\n",
    "\n",
    "def triples(val):\n",
    "    for i in range(len(val)):\n",
    "        c[val[i : i + 3]] += 1\n",
    "\n",
    "df = pd.read_csv(\"titanic_dataset.csv\", index_col=0)\n",
    "\n",
    "df.columns = (df.columns\n",
    "                .str.replace('(?<=[a-z])(?=[A-Z])', '_', regex=True)\n",
    "                .str.lower()\n",
    "             )\n",
    "\n",
    "df.name.apply(triples)\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300fb10f",
   "metadata": {},
   "source": [
    "### Other Encodings\n",
    "The _categorical_encoding_ lib is a set of scikit-learn transformers used to convert object data to numeric data.<br>\n",
    "A good feature of this lib is that it generates pandas DataFrames.<br>\n",
    "One of the algorithms implemented in this lib is a hash encoder.\n",
    "Another ordinal algorithm (_ordinal encoder_) can convert category columns that have an order into a single column of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcb4e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba223a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
