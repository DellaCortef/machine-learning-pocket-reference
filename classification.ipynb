{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0caa1a0f",
   "metadata": {},
   "source": [
    "### Intalling libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a74e34be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting distributed>2023.3.2\n",
      "  Downloading distributed-2024.8.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: locket>=1.0.0 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from distributed>2023.3.2) (1.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from distributed>2023.3.2) (6.0)\n",
      "Requirement already satisfied: psutil>=5.7.2 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from distributed>2023.3.2) (5.9.0)\n",
      "Requirement already satisfied: urllib3>=1.24.3 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from distributed>2023.3.2) (1.26.11)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from distributed>2023.3.2) (1.0.3)\n",
      "Requirement already satisfied: dask==2024.8.0 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from distributed>2023.3.2) (2024.8.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from distributed>2023.3.2) (2.4.0)\n",
      "Collecting zict>=3.0.0\n",
      "  Downloading zict-3.0.0-py2.py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click>=8.0 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from distributed>2023.3.2) (8.1.7)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from distributed>2023.3.2) (1.7.0)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from distributed>2023.3.2) (0.11.2)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from distributed>2023.3.2) (2.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from distributed>2023.3.2) (21.3)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from distributed>2023.3.2) (2.11.3)\n",
      "Requirement already satisfied: tornado>=6.0.4 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from distributed>2023.3.2) (6.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from dask==2024.8.0->distributed>2023.3.2) (8.2.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from dask==2024.8.0->distributed>2023.3.2) (2022.7.1)\n",
      "Requirement already satisfied: partd>=1.4.0 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from dask==2024.8.0->distributed>2023.3.2) (1.4.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from jinja2>=2.10.3->distributed>2023.3.2) (2.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->distributed>2023.3.2) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/dellacorte/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.13.0->dask==2024.8.0->distributed>2023.3.2) (3.8.0)\n",
      "Installing collected packages: zict, distributed\n",
      "  Attempting uninstall: zict\n",
      "    Found existing installation: zict 2.1.0\n",
      "    Uninstalling zict-2.1.0:\n",
      "      Successfully uninstalled zict-2.1.0\n",
      "  Attempting uninstall: distributed\n",
      "    Found existing installation: distributed 2022.7.0\n",
      "    Uninstalling distributed-2022.7.0:\n",
      "      Successfully uninstalled distributed-2022.7.0\n",
      "Successfully installed distributed-2024.8.0 zict-3.0.0\n"
     ]
    }
   ],
   "source": [
    "#  pip install pydotplus\n",
    "# !pip install graphviz\n",
    "# !pip install IPython\n",
    "# !pip install dtreeviz\n",
    "# !pip install -q dtreeviz\n",
    "# !pip install xgbfir\n",
    "# !pip install openpyxl\n",
    "# openpyxl.__version__\n",
    "# !pip list\n",
    "# !pip install LightGBM\n",
    "# !pip install 'dask>2023.3.2'\n",
    "# !pip install 'distributed>2023.3.2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afffba1",
   "metadata": {},
   "source": [
    "### Column description\n",
    "\n",
    "| Variável \t| Descrição \t|\n",
    "|:-:\t|:-\t|\n",
    "| PassangerID \t| ID de identificação do passageiro(a) \t|\n",
    "| Survived \t| se o passageiro(a) sobreviveu (0 = não, 1 = sim) \t|\n",
    "| Pclass \t| classe do passageiro:<br>     * **1 = primeira**,<br>     * **2 = segunda**,<br>     * **3 = terceira** \t|\n",
    "| name \t| nome do passageiro(a) \t|\n",
    "| sex \t| sexo do passageiro(a) \t|\n",
    "| age \t| idade do passageiro(a) \t|\n",
    "| Sibsp \t| número de irmão(ãs)/esposo(a) à bordo \t|\n",
    "| Parch \t| número de pais/filhos(as) à bordo \t|\n",
    "| Ticket \t| número da passagem \t|\n",
    "| Fare \t| preço da passagem \t|\n",
    "| Cabin \t| cabine \t|\n",
    "| Embarked \t| local que o passageiro(a) embarcou:<br>     * **C = Cherboug**,<br>     * **Q = Queenstown**,<br>     * **S = Southamption** \t|\n",
    "| WikiId \t| ID de identificação do passageiro(a) segundo Wikipedia \t|\n",
    "| Name_wiki \t| nome do passageiro(a) \t|\n",
    "| Age_wiki \t| idade do passageiro(a) \t|\n",
    "| Hometown \t| cidade de nascimento do passageiro(a) \t|\n",
    "| Boarded \t| cidade de embarque \t|\n",
    "| Destination \t| destino da viagem \t|\n",
    "| Lifeboat \t| identificação do bote salva-vidas \t|\n",
    "| Body \t| número de identificação do corpo \t|\n",
    "\n",
    "\n",
    "<font color='red'>**IMPORTANT**</font>\n",
    "\n",
    "The new features (the ones after 'Embarked') are very similar to the original ones but they are more up-to-date and have much fewer missing values. Therefore, users can decide on the preferred features themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696865d3",
   "metadata": {},
   "source": [
    "### Importing Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71c7b8fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data visualization\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import (\n",
    "    jointplot,\n",
    "    pairplot,\n",
    "    boxplot,\n",
    "    heatmap\n",
    ")\n",
    "\n",
    "from yellowbrick.features import (\n",
    "    Rank2D, \n",
    "    RadViz,\n",
    "    FeatureImportances,\n",
    "    ParallelCoordinates,\n",
    "    JointPlotVisualizer,\n",
    ")\n",
    "\n",
    "import dtreeviz\n",
    "\n",
    "import pydotplus\n",
    "\n",
    "from io import(\n",
    "    StringIO\n",
    ")\n",
    "\n",
    "from IPython.display import (\n",
    "    Image\n",
    ")\n",
    "\n",
    "# data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import(\n",
    "    radviz\n",
    ")\n",
    "import janitor as jn\n",
    "\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# missing values\n",
    "import missingno as msno\n",
    "\n",
    "from sklearn.impute import (\n",
    "    SimpleImputer\n",
    ")\n",
    "\n",
    "# machine learning models\n",
    "from sklearn import (\n",
    "    ensemble,\n",
    "    preprocessing,\n",
    "    tree,\n",
    "    impute,\n",
    "    model_selection\n",
    ")\n",
    "\n",
    "from sklearn.utils import (\n",
    "    resample\n",
    ")\n",
    "\n",
    "from sklearn.dummy import (\n",
    "    DummyClassifier\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split\n",
    ")\n",
    "\n",
    "from sklearn.experimental import (\n",
    "    enable_iterative_imputer\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression\n",
    ")\n",
    "\n",
    "from sklearn.naive_bayes import (\n",
    "    GaussianNB\n",
    ")\n",
    "\n",
    "from sklearn.tree import (\n",
    "    DecisionTreeClassifier,\n",
    "    export_graphviz,\n",
    "    plot_tree\n",
    ")\n",
    "\n",
    "from sklearn.neighbors import (\n",
    "    KNeighborsClassifier\n",
    ")\n",
    "\n",
    "from sklearn.naive_bayes import (\n",
    "    GaussianNB\n",
    ")\n",
    "\n",
    "from sklearn.svm import (\n",
    "    SVC\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier\n",
    ")\n",
    "\n",
    "from imblearn.over_sampling import (\n",
    "    RandomOverSampler,\n",
    ")\n",
    "\n",
    "import rfpimp\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import xgbfir\n",
    "\n",
    "# data model metrics\n",
    "from sklearn.metrics import (\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "\n",
    "from yellowbrick.classifier import (\n",
    "    ConfusionMatrix\n",
    ")\n",
    "\n",
    "from yellowbrick.model_selection import (\n",
    "    LearningCurve\n",
    ")\n",
    "\n",
    "# data prep-model\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    "    learning_curve\n",
    ")\n",
    "\n",
    "# model deploy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e460a7d",
   "metadata": {},
   "source": [
    "### Reading the Titanic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f47c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"titanic_dataset.csv\", index_col=0)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08aa73e",
   "metadata": {},
   "source": [
    "### Deleting _Class_ feature at the end\n",
    "We are deleting because is the same as _pclass_ (same result, same data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a06118",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Class', axis = 'columns')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9a0c5f",
   "metadata": {},
   "source": [
    "### Converting DataFrame Column Names to Lowercase snakecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c34f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = (df.columns\n",
    "                .str.replace('(?<=[a-z])(?=[A-Z])', '_', regex=True)\n",
    "                .str.lower()\n",
    "             )\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a274ef",
   "metadata": {},
   "source": [
    "### Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c575a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns taht do not add value\n",
    "df = df.drop(columns = ['name',\n",
    "                        'name_wiki',\n",
    "                        'wiki_id',\n",
    "                        'hometown',\n",
    "                        'destination',\n",
    "                        'ticket',\n",
    "                        'lifeboat',\n",
    "                        'body',\n",
    "                        'cabin',\n",
    "                        'fare',\n",
    "                        'age_wiki',\n",
    "                        'age'])\n",
    "\n",
    "# using get_dummies function to convert object to int\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "# dropping redundant features\n",
    "df = df.drop(columns = ['sex_male'])\n",
    "\n",
    "#remove rows with any values that are not finite (NaN or infite)\n",
    "df = df[np.isfinite(df).all(1)]\n",
    "\n",
    "# first, we need to create a series of the target feature\n",
    "df = df.apply(lambda column: column.astype(int))\n",
    "y = df.survived\n",
    "\n",
    "# then, we create a DataFrame with the attributes\n",
    "df = df.replace({True: 1, False: 0})\n",
    "X = df.drop(columns = ['survived'])\n",
    "\n",
    "# using the scikit-learn to split 30% to test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9fff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd190f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b18ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8923787",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1687db84",
   "metadata": {},
   "source": [
    "### Classification\n",
    "Classification is a _supervised learning_ method for assigning a label to a sample based on attributes. _Supervised learning_ implies that we have labels for classification or numbers for regression, which the algorithm must learn.<br><br>\n",
    "In __sklearn__, we will create a model instance and call the _.fit()_ method with the training data and labels. We can then call the _.predict_ method (or the _.predict_proba_ or _.predict_log_proba_ methods) with the model after the adjustments. Finally, to evaluate the model, we can use the _.score()_ method.<br><br>\n",
    "In general, the biggest challenge is organizing the data to use the __sklearn models__. The data (X) must be in a numpy array (m by n) format or in the pandas DataFrame format. The target data (y) is in a pandas vector or series.\n",
    "For information, there are several metrics to evaluate how well the model performs on new data.<br><br>\n",
    "The generic methods that __sklearn models__ implement are:<br>\n",
    "\n",
    "_fit(X, y[, sample_weight])_\n",
    "- Adjusts a model\n",
    "\n",
    "_predict(X)_\n",
    "- Predict classes\n",
    "\n",
    "_predict_log_proba(X)_\n",
    "- Predicts the probability lizarithm\n",
    "\n",
    "_predict_proba(X)_\n",
    "- Makes probability prediction\n",
    "\n",
    "_weight(X, y[, sample_weight])_\n",
    "- Get accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a4b53b",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Logistic Regression estimates probabilities using a logistic function (used for classification).\n",
    "\n",
    "Efficiency in execution\n",
    "- You can use *n_jobs* if you are not using the _liblinear_ solver\n",
    "\n",
    "Interpretation of Results\n",
    "- The *.coef_* attribute of the model after fitting shows the coefficients of the decision function. A change of x by one unit modifies the log odds ratio according to the coefficient. The *.intercept_* attribute is the inverse of the log odds of the base condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the model\n",
    "lr = LogisticRegression(random_state = 42)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc08479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the model performance\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec3d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict(X.iloc[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0beec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict_proba(X.iloc[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c560bb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict_log_proba(X.iloc[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1166c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.decision_function(X.iloc[[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b57803",
   "metadata": {},
   "source": [
    "#### Attributes after fitting\n",
    "*coef_*\n",
    "- Decision function coefficients\n",
    "\n",
    "*intercept_*\n",
    "- Intercept of the decision function\n",
    "\n",
    "*m_iter_*\n",
    "- Number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7438871a",
   "metadata": {},
   "source": [
    "#### The intercept is the log odds of the base condition. We can convert it back to a percentage accuracy (probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a154ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edefb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the logit inverse function\n",
    "def inv_logit(p):\n",
    "    return np.exp(p) / (1 + np.exp(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df8a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_logit(lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d0f8ba",
   "metadata": {},
   "source": [
    "#### You can inspect the coefficients. The inverse ligit of the coefficients gives us the proportion of positive cases. In other words, for each of the attributes, we can increase or reduce the chance of survival (in the case of the Titanic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2b8b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining de the columns to be analysed\n",
    "cols = X.columns\n",
    "\n",
    "for col, val in sorted(\n",
    "    zip(cols, lr.coef_[0]),\n",
    "    key = lambda x: x[1],\n",
    "    reverse = True,\n",
    "):\n",
    "    print(\n",
    "        f\"{col:10}{val:10.3f} {inv_logit(val):10.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b32759",
   "metadata": {},
   "source": [
    "#### We can use yellowbrick lib to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ac946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (6, 4))\n",
    "\n",
    "fi_viz = FeatureImportances(lr)\n",
    "\n",
    "fi_viz.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f41da9",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "Naive Bayes is a probabilistic classifier that assumes independence between data attributes. It is popular for text classification applications, such as *spam* identification.<br><br>\n",
    "An advantage of this model is that, by assuming independence between attributes, it is capable of training a model with a small number of samples. A disadvantage is that the model will not be able to capture interactions between attributes.<br><br>\n",
    "__GaussianNB__\n",
    "- Assumes a normal distribution\n",
    "\n",
    "__MultinomialNB__\n",
    "- Used for discrete occurrence counters\n",
    "\n",
    "__BernoulliNB__\n",
    "- Used for discrete Boolean attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09297f6b",
   "metadata": {},
   "source": [
    "__Model properties__\n",
    "\n",
    "_Pre processing_\n",
    "- it is assumed that the data are independent. Performance should be better if we remove collinear columns. For continuous numeric data, it may be better to separate into bins.\n",
    "\n",
    "_To avoid overfitting_\n",
    "- exhibits high bias and low variance (ensembles will not reduce variance).\n",
    "\n",
    "_Interpretation of results_\n",
    "- percentage is the probability of a sample belonging to a class based on priors (prior knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720d1ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8bcd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075f7320",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.predict(X.iloc[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a051d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.predict_proba(X.iloc[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3092f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.predict_log_proba(X.iloc[[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974004a9",
   "metadata": {},
   "source": [
    "__Instance Parameters__\n",
    "\n",
    "_priors_\n",
    "- prior probabilities (prior) of classes\n",
    "\n",
    "_var_smoothing = 1e-9_\n",
    "- Adding to variance for stable calculations\n",
    "\n",
    "<br><br>\n",
    "__Attributes after adaptation__\n",
    "\n",
    "_class_prior_\n",
    "- Class probabilities\n",
    "\n",
    "_class_count_\n",
    "- Class counters\n",
    "\n",
    "_theta_\n",
    "- Average and each column per class\n",
    "\n",
    "_sigma_\n",
    "- Variance of each column by class\n",
    "\n",
    "_epsilon_\n",
    "- Value to be added for each variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288628aa",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)\n",
    "An SVM is an algorithm that tries to adapt a line between different classes in order to maximize the distance from the line to the class points.<br><br>\n",
    "__Model properties__<br>\n",
    "_Efficiency in execution_\n",
    "- The scikit-learn implementation is O(n4), so it may be difficult to scale to larger sizes. Using a linear kernel or the _LinearSVC_ model can improve execution performance, perhaps at the expense of accuracy.\n",
    "\n",
    "_Data pre-processing_\n",
    "- The algorithm is not scale invariant. Standardizing data is highly recommended.\n",
    "\n",
    "_To avoid overfitting_\n",
    "- Parameter C (penalty parameter) controls regularization. A smaller value allows for a smaller margin in the hyperplane. A larger value for _gamma_ will tend to overfit the training data.\n",
    "\n",
    "_Interpretation of results_\n",
    "- We should inspect .support_vectors_, although they can be difficult to explain. With linear kernels you will be able to inspect the .coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5245dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(random_state = 42, probability = True)\n",
    "\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d2a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee3e884",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.predict(X.iloc[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0bdd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.predict_proba(X.iloc[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41a6092",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.predict_log_proba(X.iloc[[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43819bf1",
   "metadata": {},
   "source": [
    "We use _probability_ = True to obtain the probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce163cb0",
   "metadata": {},
   "source": [
    "__Instance Parameters__\n",
    "\n",
    "_C=1.0_\n",
    "- penalty parameter. The lower the value, the narrower the decision boundary (more overfit).\n",
    "\n",
    "*cache_size*\n",
    "- Cache size (MB). Increasing this value can improve training time on large data sets.\n",
    "\n",
    "*class_weight*\n",
    "- Dictionary or _balanced_\n",
    "\n",
    "*coef*\n",
    "- Independent term for polynomial and sigmoid kernels\n",
    "\n",
    "*degree*\n",
    "- Degree for polynomial kernel\n",
    "\n",
    "*max_iter*\n",
    "- maximum number of iterations for the solver. -1 indicates there are no limits\n",
    "\n",
    "*probability*\n",
    "- False activates probability estimation. Makes training slower\n",
    "\n",
    "*tol*\n",
    "- Stop tolerance\n",
    "\n",
    "*support_*\n",
    "- Support vector indices\n",
    "\n",
    "*support_vectors_*\n",
    "- Support vectors\n",
    "\n",
    "*n_support_vectors_*\n",
    "- Number of support vectors per class\n",
    "\n",
    "*coef_*\n",
    "- Coefficients for kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bf6062",
   "metadata": {},
   "source": [
    "### K vizinhos mais próximos (KNN)\n",
    "The KNN (K-Nearest Neighbor) algorithm performs the classification based on the distance to some training samples (k). The family of algorithms is called _instance-based learning_, as there are no parameters to learn. The model assumes that the distance is sufficient to make the inference.<br><br>\n",
    "The tricky part is selecting the appropriate value of _K_. Furthermore, the curse of dimensionality can hinder distance metrics, as there will be little difference between the closest and most distant neighbors in the case of more dimensions.<br><br>\n",
    "__Model properties__<br>\n",
    "_Data pre-processing_\n",
    "- Distance-based calculations perform better if there is standardization\n",
    "\n",
    "_To avoid overfitting_\n",
    "- Elevate *n_neighbors*. Change *p* to L1 or L2 metric\n",
    "\n",
    "*Interpretation of results*\n",
    "- Interprets the k nearest neighbors for the sample (using the .kneighbors method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031b2e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "knc = KNeighborsClassifier()\n",
    "\n",
    "knc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851f8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "knc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0b7d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "knc.predict(X.iloc[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18e01f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "knc.predict_proba(X.iloc[[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6f81d3",
   "metadata": {},
   "source": [
    "__Attributes__<br>\n",
    "*leaf size*\n",
    "- Used for tree-based algorithms\n",
    "\n",
    "*metric*\n",
    "- Distance metric\n",
    "\n",
    "*metric_params*\n",
    "- Additional parameter dictionary for custom metrics function\n",
    "\n",
    "*n_jobs*\n",
    "- Number of CPUs\n",
    "\n",
    "*n_neighbors*\n",
    "- Number of neighbors\n",
    "\n",
    "*weights*\n",
    "- Because it gives *distance*, in which case closer points will have more influence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60b83f2",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "We can use a question and answer process to predict a target class. The advantages of this model include support for non-numeric features, little need for data preparation, support for dealing with non-linear relationships, and finally, we can obtain the importance of the attributes at the end of the model.<br><br>\n",
    "__Model properties__<br>\n",
    "*Efficiency in execution*\n",
    "- goes through each of the attributes and sorts all *n* samples;\n",
    "\n",
    "*Data pre-processing*\n",
    "- it is not necessary to climb. You need to get rid of missing values ​​and convert them into numerical data;\n",
    "\n",
    "*To avoid overfitting*\n",
    "- set *max_depth* to a smaller number and increase *min_impurity_decrease*\n",
    "\n",
    "*Interpretation of results*\n",
    "- because there are steps, a tree is bad at dealing with linear relationships. The tree is also extremely dependent on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667c0839",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(\n",
    "    random_state=42, max_depth=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60250bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d35df40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1914ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.predict(X.iloc[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14c3fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.predict_proba(X.iloc[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2034c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.predict_log_proba(X.iloc[[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ed7c17",
   "metadata": {},
   "source": [
    "__Attributes__<br>\n",
    "*class_weight*\n",
    "- class weights in a dictionary. *balanced* defines values in inverse proportion to class frequencies\n",
    "\n",
    "*criterion='gini'*\n",
    "- separation function, *gini* or *entropy*\n",
    "\n",
    "*max_depth=None*\n",
    "- depth of the tree\n",
    "\n",
    "*max_features=None*\n",
    "- number of attributes to be analyzed for separation. The default value is all\n",
    "\n",
    "*max_leaf_nodes=None*\n",
    "- limitates the number of leaves\n",
    "\n",
    "*min_impurity_decrease=0.0*\n",
    "- separates a node if the separation decreases the impurity by an amount greater than or equal to the defined\n",
    "\n",
    "*min_samples_leaf*\n",
    "- minimum number of samples required to separate a node<br><br>\n",
    "\n",
    "\n",
    "**Attributes after adjustment**\n",
    "\n",
    "*classes_*\n",
    "- class label\n",
    "\n",
    "*feature_importances_*\n",
    "- array with the importance of each feature\n",
    "\n",
    "*n_classes_*\n",
    "- number of classes\n",
    "\n",
    "*n_features_*\n",
    "- number of attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcda1e4",
   "metadata": {},
   "source": [
    "#### Viewing the implemented tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1945e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dot_data = StringIO()\n",
    "\n",
    "tree.export_graphviz(\n",
    "    dt,\n",
    "    out_file=dot_data'\n",
    "    feature_names=[\"Died\", \"Survived\"],\n",
    "    filled=True\n",
    ")\n",
    "\n",
    "g = pydotplus.graph_from_dot_data(\n",
    "    dot_data.getvalue()\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618f4a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pydotplus\n",
    "from io import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "dot_data = StringIO()\n",
    "tree.export_graphviz(\n",
    "    dt,\n",
    "    out_file=dot_data,\n",
    "    feature_names=X.columns,\n",
    "    class_names=[\"Died\", \"Survived\"],\n",
    "    filled=True'\n",
    ")\n",
    "g = pydotplus.graph_from_dot_data(\n",
    "    dot_data.getvalue()\n",
    ")\n",
    "\n",
    "g.write(\"py-projectsdata-science-projectsmachine-learning-pocket-reference-tree_classifier_view.png\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98955627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image(g.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1752ee6",
   "metadata": {},
   "source": [
    "#### Visualizing DecisionTree, which provides valuable visual insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d2f3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tree.plot_tree to visualize DecisionTreeClassifier\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(dt\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec314c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using plot_tree to view DecisionTreeClassifier\n",
    "plot_tree(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a29f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "plot_tree(dt)\n",
    "\n",
    "fig.savefig(\"images/classification_decision_tree.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c73b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using dtreeviz to view DecisionTreeClassifier\n",
    "viz = dtreeviz.model(dt, X, y,\n",
    "               target_name=\"survived\",\n",
    "               feature_names=X.columns,\n",
    "               class_names=[\"died\", \"survived\"]\n",
    ")\n",
    "\n",
    "viz.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be26419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using text_representation to view DecisionTreeClassifier\n",
    "text_representation = tree.export_text(dt)\n",
    "print(text_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746c43c3",
   "metadata": {},
   "source": [
    "#### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b109ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, val in sorted(\n",
    "    zip(X.columns, dt.feature_importances_),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:5]:\n",
    "    print(f\"{col:10}{val:10.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ded731",
   "metadata": {},
   "source": [
    "#### We can use Yellowbrick to visualize the Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67a0f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "fi_viz = FeatureImportances(dt)\n",
    "\n",
    "fi_viz.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b517e2d9",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "A Random Forest is a set of decision trees. She uses *bagging* to correct the trees' tendency to overfitting. Random Forest provides us with feature importances by calculating the average importance across all trees.<br><br>\n",
    "**Model Properties**<br>\n",
    "*Efficiency in execution*\n",
    "- Create *j* using *n_jobs*. The complexity of each tree is O(mn log n(, where *n* is the number of samples and *m* the number of attributes.\n",
    "\n",
    "*Data pre-processing*\n",
    "- it is not necessary\n",
    "\n",
    "*To avoid overfitting*\n",
    "- add more trees (*n_estimators*). Use a smaller value for *max_depth*\n",
    "\n",
    "*Interpretation of results*\n",
    "- has support for feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87790f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d1ab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0446da",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca70428",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.predict(X.iloc[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3498283",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.predict_proba(X.iloc[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730266d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.predict_log_proba(X.iloc[[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca5f93a",
   "metadata": {},
   "source": [
    "**Instance Parameters**\n",
    "\n",
    "*class_weight*\n",
    "- class weights in a dictionary. *balanced* defines values in inverse proportion to class frequencies\n",
    "\n",
    "*criterion='gini'*\n",
    "- separation function, *gini* or *entropy*\n",
    "\n",
    "*max_depth=None*\n",
    "- depth of the tree\n",
    "\n",
    "*max_features=None*\n",
    "- number of attributes to be analyzed for separation. The default value is all\n",
    "\n",
    "*max_leaf_nodes=None*\n",
    "- limitates the number of leaves\n",
    "\n",
    "*min_impurity_decrease=0.0*\n",
    "- separates a node if the separation decreases the impurity by an amount greater than or equal to the defined\n",
    "\n",
    "*min_samples_leaf*\n",
    "- minimum number of samples required to separate a node\n",
    "\n",
    "*n_estimators*\n",
    "- number of trees in the forest\n",
    "\n",
    "*n_jobs*\n",
    "- number of jobs to be carried out for adaptation and prediction<br><br>\n",
    "\n",
    "\n",
    "**Attributes after Adjustment**\n",
    "\n",
    "*classes_*\n",
    "- class label\n",
    "\n",
    "*feature_importances_*\n",
    "- array with the importance of each feature\n",
    "\n",
    "*n_classes_*\n",
    "- number of classes\n",
    "\n",
    "*n_features_*\n",
    "- number of attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4401b48a",
   "metadata": {},
   "source": [
    "#### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c7c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, val in sorted(\n",
    "    zip(X.columns, rf.feature_importances_),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:6]:\n",
    "    print(f\"{col:10}{val:10.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dc9f56",
   "metadata": {},
   "source": [
    "The random forest classifier calculates the importance of attributes by determining the *average impurity decrease* for each attribute. Attributes that reduce classification uncertainty receive higher scores.<br><br>\n",
    "These numbers may become inaccurate if the attributes vary in scale or cardinality of the category columns. A more reliable score is the *importance of the permutation*. A more reliable method is *discarded column importance*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df77703",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2 = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf2.fit(X_train, y_train)\n",
    "\n",
    "rfpimp.importances(\n",
    "    rf2, X_test, y_test\n",
    ").Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f4fe5f",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "XGBoost is a popular library. It creates a weak tree and then \"improves\" subsequent trees (boosting) in order to reduce residual errors. The algorithm attempts to capture and handle any patterns in the errors until they appear to be random.<br><br>\n",
    "**Model Properties**<br>\n",
    "*Efficiency in execution*\n",
    "- XGBoost can run in parallel. Using the *n_jobs* function to inform the number of CPUs.\n",
    "\n",
    "*Data pre-processing*\n",
    "- no need to scale in tree-based models. Category data must be encoded\n",
    "\n",
    "*To avoid overfitting*\n",
    "- the parameter *early_stopping_rounds=N* can be defined to stop training if there are no improvements after N rounds.\n",
    "\n",
    "*Interpretation of results*\n",
    "- it is possible to include the importance of attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4729afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# early_stopping_rounds shoub be passed on XGBClassifier()\n",
    "xgb_class = xgb.XGBClassifier(random_state=42, early_stopping_rounds=10)\n",
    "\n",
    "xgb_class.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_test, y_test)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfce2460",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_class.predict(X.iloc[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edac4b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_class.predict_proba(X.iloc[[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6b63ea",
   "metadata": {},
   "source": [
    "**Instance parameters**<br>\n",
    "*max_depth*\n",
    "- maximum depth\n",
    "\n",
    "*learning_rate*\n",
    "- learning rate for boosting. After each execution step, the newly added weights are scaled according to this factor. The lower the value, the more conservative it will be, but more trees will be needed to converge. In the *.train* call, we can pass a *learning_rates* parameter, which is a list of rates in each round\n",
    "\n",
    "*n_estimators*\n",
    "- number of rounds or improved trees\n",
    "\n",
    "*n-jobs*\n",
    "- number of threads to be used\n",
    "\n",
    "*gamma*\n",
    "- controls pruning. Ranges from 0 to infinity. Minimum loss reduction required to separate one more sheet. The higher the gamma value, the more conservative it will be.\n",
    "\n",
    "*subsample*\n",
    "- fraction of samples to be used in the next round\n",
    "<br><br>\n",
    "\n",
    "**Attributes** <br>\n",
    "*coef_*\n",
    "- coefficients for gblinear learners\n",
    "\n",
    "*feature_importances_*\n",
    "- importance of attributes. The importance of attributes is the average gain across all nodes where the attribute is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427e8299",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, val in sorted(\n",
    "    zip(\n",
    "        X.columns,\n",
    "        xgb_class.feature_importances_,\n",
    "    ),\n",
    "    key=lambda X: X[1],\n",
    "    reverse=True\n",
    ")[:6]:\n",
    "    print(f\"{col:10}{val:10.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353b644a",
   "metadata": {},
   "source": [
    "#### Generating a graph of the importance of attributes with XGBoost\n",
    "XGBoost has *importance_type* parameter. The default value is *weight*, which is the number of times. an attribute appears in a tree. It can be *gain*, which would be the average gain when the attribute is used, and it can also be *cover*, which would be the number of samples affected by a separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83631729",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "xgb.plot_importance(xgb_class, ax=ax)\n",
    "\n",
    "fig.savefig(\"images/classification_xgboost_feature_importances.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8868990d",
   "metadata": {},
   "source": [
    "#### Generating a graph of the importance of attributes with Yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f647ad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "fi_viz = FeatureImportances(xgb_class)\n",
    "\n",
    "fi_viz.fit(X, y)\n",
    "\n",
    "fig.savefig(\"images/classification_yellowbrick_xgboost_feature_importances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ddcb0a",
   "metadata": {},
   "source": [
    "#### XGBoost offers a textual representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94946ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = xgb_class.get_booster()\n",
    "\n",
    "print(booster.get_dump()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d41e21",
   "metadata": {},
   "source": [
    "The value on the sheet is the score for class 1. It can be converted to a probability using the logistic function.\n",
    "\n",
    "Next, we will plot the graphical version of the first model tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df24f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "xgb.plot_tree(xgb_class, ax=ax, num_trees=0)\n",
    "\n",
    "fig.savefig(\"images/classification_first_tree_xgboost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551eef19",
   "metadata": {},
   "source": [
    "#### xgbfir package\n",
    "The xgb fit package is a lib developed based on XGBoost. This lib provides several measures related to the importance of attributes. Its unique feature is that it provides these measurements over columns, and over pairs of columns as well, so that you can see the interactions. Furthermore, we can obtain information about interactions between column cracks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bc12db",
   "metadata": {},
   "source": [
    "**Measurements**<br><br>\n",
    "*Gain*\n",
    "- total gain of each attribute or interaction between attributes\n",
    "\n",
    "*FScore*\n",
    "- number of possible separations in an attribute or interaction between attributes\n",
    "\n",
    "*wFScore*\n",
    "- number of possible separations in an attribute or interaction between attributes, with weights assigned according to the probability of the separations occurring\n",
    "\n",
    "*Average wFScore*\n",
    "- wFScore divided by FScore\n",
    "\n",
    "*Average Gain*\n",
    "- gin divided by FScore\n",
    "\n",
    "*Expected Gain*\n",
    "- total gain for each attribute or interaction between attributes, with weights according to the probability of obtaining the gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908c1950",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbfir.saveXgbFI(\n",
    "    xgb_class,\n",
    "    feature_names=X.columns,\n",
    "    OutputXlsxFile=\"temp/measurements_xgb_fir.xlsx\",\n",
    ")\n",
    "\n",
    "# selecting 3 main attributes\n",
    "pd.read_excel(\"temp/measurements_xgb_fir.xlsx\").head(3).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a884f3c",
   "metadata": {},
   "source": [
    "From this table, we can see that *sex_female* has a high position in terms of **Gain**, **Average wDScore**, **Average gain** and **Expected gain**, while *sib_sp* is highlights in **FScore** and **wFScore**<br><br>\n",
    "Let's analyze the pairs to see the interactions between the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6691f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel(\n",
    "       \"temp/measurements_xgb_fir.xlsx\",\n",
    "        sheet_name='Interaction Depth 1',\n",
    ").head(5).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4702f5d2",
   "metadata": {},
   "source": [
    "In this case, we can see that the two main interactions involve the *sex_female* and *pclass* column. If we had to choose just 2 attributes for our model, we would probably choose *sex_female* and *pclass*<br><br>\n",
    "Next, we will analyze the column triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c7cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel(\n",
    "       \"temp/measurements_xgb_fir.xlsx\",\n",
    "        sheet_name='Interaction Depth 2',\n",
    ").head(5).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8409d3e8",
   "metadata": {},
   "source": [
    "### LightGBM\n",
    "LightGBM is a Microsoft implementation. It uses a sampling method to deal with continuous values. This allows for faster tree creation (compared to XGBoost), as well as reducing memory usage.<br>\n",
    "LightGBM also creates trees in depth by leaves, rather than levels. Because of this, instead of using *max_depth* to control overfitting, we use *num_leaves*.<br><br>\n",
    "**Model properties**<br><br>\n",
    "*Efficiency in execution*\n",
    "- can take advantage of multiple CPUs\n",
    "\n",
    "*Data pre-processing*\n",
    "- has some support for encoding category columns as integers\n",
    "\n",
    "*To avoid overfitting*\n",
    "- reduce *num_leaves*, increase *min_data_in_leaf* and use *min_gain_to_split* with **lambda_L1** and **lambda_L2**\n",
    "\n",
    "*Interpretation of results*\n",
    "- the importance of attributes is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc9d63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_class = lgb.LGBMClassifier(\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4bb4684",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vb/yxjjx1m94n93ty4t50m8f2b40000gn/T/ipykernel_82720/1297751177.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlgbm_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "lgbm_class.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57387cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
